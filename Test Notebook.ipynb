{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d4c16f-1a1b-43d5-ae29-53af76b33fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fc81b4-655f-45d2-8018-d2d214fb8ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random, string, math\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefd88a3-1e28-4c65-8242-847f957c7e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "with open(\"wordle_words.txt\", \"r\") as f:\n",
    "    words = [line.strip() for line in f if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4319c9cc-072f-4565-a850-31dbcddd11ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(gold: str, tries: list[str]):\n",
    "    '''\n",
    "    The model input will be the sequence of previous tries, tokenized as follows:\n",
    "    One token per letter, plus an EoS token labeled 0.\n",
    "    Gray letters will be labeled 1-26, yellow letters 27-52, and green letters 53-78, for a total vocabulary size of 79.\n",
    "    Example: tokenize('hello', ['sales', 'round']) = [19, 1, 64, 31, 19, 18, 41, 21, 14, 4, 0].\n",
    "    Here, for example, the 'l' in 'sales' (the third token) is the correct letter in the correct position; as 'l' is the 12th letter of the alphabet, it will be assigend token 12+52=64.\n",
    "    We work under the assumption that all words are 5 letters, so the tokenized sequence will always have length 1 (mod 5).\n",
    "    '''\n",
    "    tokenized_seq = []\n",
    "    for word in tries:\n",
    "        remaining = {}\n",
    "        for c in gold:\n",
    "            remaining[c] = remaining.get(c, 0) + 1\n",
    "\n",
    "        marks = [0] * 5 #This will keep track of whether each position in word is gray, yellow, or green.\n",
    "        for i, ch in enumerate(word):\n",
    "            if ch == gold[i]:\n",
    "                marks[i] = 2\n",
    "                remaining[ch] -= 1\n",
    "\n",
    "        for i, ch in enumerate(word):\n",
    "            if marks[i] == 0 and remaining.get(ch, 0) > 0:\n",
    "                marks[i] = 1\n",
    "                remaining[ch] -= 1\n",
    "\n",
    "        for i, ch in enumerate(word):\n",
    "            base = ord(ch) - 96 #Magic number, so that the base of 'a' is 1.\n",
    "            if marks[i] == 2:\n",
    "                tokenized_seq.append(base + 26 * 2)\n",
    "            elif marks[i] == 1:\n",
    "                tokenized_seq.append(base + 26)\n",
    "            else:\n",
    "                tokenized_seq.append(base)\n",
    "\n",
    "    tokenized_seq.append(0)\n",
    "    return tokenized_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ef7b2b-f22b-46cb-881d-fd01b5b4f41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordlePlayer(nn.Module):\n",
    "    '''\n",
    "    A pretty standard transformer model. 6 layers, dimension 128, 8 attention heads. Outputs logits with length equal to self.num_words, from which the final word choice is chosen.\n",
    "    '''\n",
    "    def __init__(self, tokenizer, num_words: int):\n",
    "        super().__init__()\n",
    "        self.num_words = num_words\n",
    "        self.tokenize = tokenizer\n",
    "        \n",
    "        vocab_size = 79\n",
    "        d_model = 128\n",
    "        num_heads = 8\n",
    "        num_layers = 6\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(6, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, batch_first=True, activation=\"gelu\", norm_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.cls_head = nn.Linear(d_model, self.num_words)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.tok_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.pos_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.cls_head.weight)\n",
    "        nn.init.zeros_(self.cls_head.bias)\n",
    "\n",
    "    def next_guess(self, correct_word: str, tries_so_far: list[str], topP: float=1.0):\n",
    "        #We implement topP to avoid particularly unlikely guesses.\n",
    "        #TopP here is a better choice than topK because there's generally going to be a large number of viable early guesses, so this helps keep that variety.\n",
    "        tokenized_tries = self.tokenize(correct_word, tries_so_far)\n",
    "        x = torch.as_tensor(tokenized_tries, device=device)\n",
    "        logits = self.forward(torch.as_tensor(tokenized_tries).unsqueeze(0))\n",
    "\n",
    "        if 0.0 < topP < 1.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
    "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "            to_remove = cumulative_probs > topP\n",
    "            to_remove[..., 1:] = to_remove[..., :-1].clone()\n",
    "            to_remove[..., 0] = False\n",
    "\n",
    "            sorted_logits = sorted_logits.masked_fill(to_remove, float('-inf'))\n",
    "            filtered_logits = torch.full_like(logits, float('-inf'))\n",
    "            filtered_logits.scatter_(1, sorted_indices, sorted_logits)\n",
    "            logits = filtered_logits\n",
    "        \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        choice = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        return words[choice]\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = x.to(device)\n",
    "        _, L = x.shape\n",
    "\n",
    "        '''\n",
    "        Now here's a great little feature of this model:\n",
    "        Because the order of the guesses doesn't actually matter, all that matters per-token is the position of that letter within its guess (i.e. is it the first letter, the second, etc.)\n",
    "        So we actually only need *five* positional embeddings (plus one for the EoS token), which repeat cyclically.\n",
    "        The great thing about this is not only do we have fewer parameters to train, but also we can expect the model to run reasonably well on inputs larger than those on which it's trained.\n",
    "        The training data only features up to 5 previous guesses like real Wordle, but with these periodic embeddings, we can actually run the model on an indefinite number of prior guesses.\n",
    "        This means we can measure how long it takes to guess the correct word, even if that runs longer than the 6 guesses in our training data.\n",
    "        '''\n",
    "        pos = [0, 1, 2, 3, 4] * ((L - 1) // 5)\n",
    "        pos += [5]\n",
    "        pos = torch.as_tensor(pos).to(device)\n",
    "        h = self.tok_emb(x) + self.pos_emb(pos)\n",
    "        h = self.encoder(h)\n",
    "\n",
    "        eos_repr = h[:, -1, :]\n",
    "        logits = self.cls_head(eos_repr)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8c8218-6aa5-4776-acb3-2ece905fe27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WordlePlayer(tokenize, len(words)).to(device)\n",
    "ckpt_path = \"models/phase2.pt\"\n",
    "with torch.no_grad():\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5c22ac-36a1-489f-bf69-f407285fbfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guesses_until_correct(model, gold: str, max_guesses: int):\n",
    "    tries = []\n",
    "    num_tries = 0\n",
    "    while gold not in tries:\n",
    "        num_tries += 1\n",
    "        next_guess = model.next_guess(gold, tries, topP=0.95)\n",
    "        tries.append(next_guess)\n",
    "        if num_tries > max_guesses:\n",
    "            return tries\n",
    "    if gold in tries:\n",
    "        return tries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ca3689-90ac-48c6-ad10-e8a398afad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_word = \"hello\"\n",
    "max_guesses = 100\n",
    "guesses = guesses_until_correct(model, correct_word, max_guesses=max_guesses)\n",
    "for guess in guesses:\n",
    "    print(guess)\n",
    "if len(guesses) == max_guesses:\n",
    "    print(\"Timed out!\")\n",
    "else:\n",
    "    print(f\"Guessed in {len(guesses)} tries!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e51c5d2-cbb6-4f1e-ba9a-4dda88c891a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, word_list, num_trials=1000):\n",
    "    total_guesses = 0\n",
    "    total_successes = 0\n",
    "    for j in tqdm(range(num_trials)):\n",
    "        word = random.choice(word_list)\n",
    "        num_guesses = len(guesses_until_correct(model, word, max_guesses = 100))\n",
    "        total_guesses += num_guesses\n",
    "        if num_guesses <= 6:\n",
    "            total_successes += 1\n",
    "\n",
    "    avg_guesses = total_guesses / num_trials\n",
    "    success_rate = total_successes / num_trials\n",
    "    return avg_guesses, success_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21091b54-529c-44dc-babb-e57cd7758312",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_guesses, success_rate = evaluate_model(model, word_list=words, num_trials=1000)\n",
    "print(f\"Average number of guesses: {avg_guesses}\")\n",
    "print(f\"Success rate: {success_rate}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
