# Wordle-Playing Transformer
We train a small transformer model to play Wordle.

## How to use it
Simply download and run the Test Notebook. The `guesses_until_correct` function lets you give a (hidden) correct word and a maximum number of guesses, and you can see what the model guesses. You can also evaluate the model yourself on the average number of guesses and success rate over a random spread of words using `evaluate_model`.
## How it works
The architecture is a very standard 6-layer, 128-dimensional transformer model which takes as input the list of previous guesses and outputs the next guess. The inputs are tokenized by letter, with gray, yellow, and green letters being assigned different tokens, while the output is a list of logits in the ~15k possible words that are acceptable in Wordle (see wordle_words.txt). 

An interesting feature of this model is that, since the order of guesses doesn't matter, we only need positional embeddings to distinguish the position of a letter inside a word, and that's it. So we only have five positional embeddings (plus one for the EoS token), corresponding to the five positions of a letter within a word. This in turn means we can run the model effectively on guess sequences much larger than those we trained on; in fact, our training data only goes up to 5 prior guesses, while we can see when testing that the model still "works" with more than that.

The training data was carefully chosen and refined to try to teach the model as much as possible. In Phase 1, we used a combination of random guesses, guesses which priortize using unused letters, guesses sharing rare letters with the correct word, guesses which prioritize using letters that *aren't* in the correct word, guesses which are just one letter away from the correct word, and one guess which is the correct word itself to teach spelling. Phase 2 was almost the same, but we swapped out the random guesses in favor of guesses generated by the pre-trained Phase 1 model.

In Phase 1, we used a scheduler which bounced between a learning rate of 5e-4 and 5e-5 at regular intervals (i.e. cosine annealing with warm restarts and `T_mult=1`), which we found to be more effective than a monotonic learning rate. In Phase 2, we started with the pre-trained Phase 1 model, and just used a basic fixed 1e-6 learning rate with a short warm-up.

## Examples/Results
Using phase2.pt, we get the following example guess sequence with correct word "hello":

``` munga, solei, ceroc, bello, cello, hello ```

So we got it in six tries! It's not bad, but we do see limitations, which here means logically inconsistent guesses. For example, it finds the middle 'l' in "solei" but temporarily abandons it in "ceroc." It should also know "cello" isn't correct because the "ceroc" guess should have indicated there are no 'c's.

Here's a worse example, with correct word "homes":

``` scuba, selfs, zymes, dores, pomes, tomes, momes, fomes, momes, nomes, homes ```

This is an unfortunately somewhat common "catastrophic" failure, where the model seems to get all but one letter correct and then flouders around, possibly re-guessing the same word multiple times. I've tried mitigating this by adding these sort of examples to the training data, but it only seems to have partially worked.

The evaluation data for phase2.pt is: ` Average number of guesses: 6.759 `, `Success rate: 0.644 `. I'm sure this can continue to be improved.

## Further work
Right now, the model makes each guess in an attempt to always guess the correct word, with no other considerations. A clear way to improve the model would be to bias the model towards high information guesses (i.e. starting with common letters, for example, or avoiding guesses that have a higher chance of triggering the above off-by-one catastrophe). We could do this with reinforcement learning, where the reward function is the number of tries it takes the model to guess the correct word. Look out for future updates!