{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e62f18e-f538-4a37-aae4-4d89f77843a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30d59dd-679b-45e9-aa08-58294485f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random, string, math\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025db464-176e-4bd5-870b-0209ac3a4eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "with open(\"wordle_words.txt\", \"r\") as f:\n",
    "    words = [line.strip() for line in f if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5543831d-b86d-454a-86b2-103449842105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(gold: str, tries: list[str]):\n",
    "    '''\n",
    "    The model input will be the sequence of previous tries, tokenized as follows:\n",
    "    One token per letter, plus an EoS token labeled 0.\n",
    "    Gray letters will be labeled 1-26, yellow letters 27-52, and green letters 53-78, for a total vocabulary size of 79.\n",
    "    Example: tokenize('hello', ['sales', 'round']) = [19, 1, 64, 31, 19, 18, 41, 21, 14, 4, 0].\n",
    "    Here, for example, the 'l' in 'sales' (the third token) is the correct letter in the correct position; as 'l' is the 12th letter of the alphabet, it will be assigend token 12+52=64.\n",
    "    We work under the assumption that all words are 5 letters, so the tokenized sequence will always have length 1 (mod 5).\n",
    "    '''\n",
    "    tokenized_seq = []\n",
    "    for word in tries:\n",
    "        remaining = {}\n",
    "        for c in gold:\n",
    "            remaining[c] = remaining.get(c, 0) + 1\n",
    "\n",
    "        marks = [0] * 5 #This will keep track of whether each position in word is gray, yellow, or green.\n",
    "        for i, ch in enumerate(word):\n",
    "            if ch == gold[i]:\n",
    "                marks[i] = 2\n",
    "                remaining[ch] -= 1\n",
    "\n",
    "        for i, ch in enumerate(word):\n",
    "            if marks[i] == 0 and remaining.get(ch, 0) > 0:\n",
    "                marks[i] = 1\n",
    "                remaining[ch] -= 1\n",
    "\n",
    "        for i, ch in enumerate(word):\n",
    "            base = ord(ch) - 96 #Magic number, so that the base of 'a' is 1.\n",
    "            if marks[i] == 2:\n",
    "                tokenized_seq.append(base + 26 * 2)\n",
    "            elif marks[i] == 1:\n",
    "                tokenized_seq.append(base + 26)\n",
    "            else:\n",
    "                tokenized_seq.append(base)\n",
    "\n",
    "    tokenized_seq.append(0)\n",
    "    return tokenized_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0349aa-d7fb-4edc-8c3f-f654b623e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataset(Dataset):\n",
    "    def __init__(self, words: list[str], tokenizer):\n",
    "        self.words = words\n",
    "        self.tokenizer = tokenizer\n",
    "        self.words_by_letter = {}\n",
    "        self.words_differing_by_one_letter = {}\n",
    "        #A dictionary whose keys are the elements of self.words, and whose corresponding value is the list of words in self.words \n",
    "        for word in self.words:\n",
    "            for letter in word:\n",
    "                self.words_by_letter.setdefault(letter, []).append(word)\n",
    "                #A dictionary whose keys are letters, and whose corresponding value is the list of words containing that letter.\n",
    "        for word in self.words:\n",
    "            self.words_differing_by_one_letter[word] = []\n",
    "            for other_word in self.words:\n",
    "                same_letter_count = 0\n",
    "                for i in range(5):\n",
    "                    if word[i] == other_word[i]:\n",
    "                        same_letter_count += 1\n",
    "                if same_letter_count == 4:\n",
    "                    self.words_differing_by_one_letter[word].append(other_word)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words) * 18 #Each word will be the gold in 18 items.\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        '''\n",
    "        The dataset is structured as follows: Each word is the gold in 18 items.\n",
    "        The first six items will feature random guesses chosen uniformly from self.words, the number of such guesses starting at 0 and increasing by 1 each time.\n",
    "        The exception to this: 25% of the time, the first guess will be a word that differs in just one letter from the gold. 2% of the time, the second guess will also be this.\n",
    "        This is because, from initial testing, the model can sometimes get stuck in a loop guessing the same word that's wrong in only one letter.\n",
    "        The seventh item will feature a single guess which is the correct word.\n",
    "        This is so that the model can learn the actual correct spellings of words, since the output tokenization is word-level rather than letter-level.\n",
    "        The eight to twelfth items will feature guesses that prioritize using a large number of unguessed letters. The intention is that these guess sequences will be high-information.\n",
    "        The exception to this: If the gold has at least one uncommon letter, the first guess will be another word that has that letter.\n",
    "        The final six items will feature guesses that prioritize guessing letters that *aren't* in the gold. This is so the model learns to extract information from gray letters, something it tends to struggle a bit with.\n",
    "        '''\n",
    "        '''\n",
    "        Key feature of this dataset: The number of tries (i.e. the length of the input) depends only on idx % 18. This will be very useful in batching.\n",
    "        '''\n",
    "        gold_idx = idx // 18\n",
    "        gold = self.words[gold_idx]\n",
    "        tries = []\n",
    "        idx_mod18 = idx % 18\n",
    "        if idx_mod18 < 6:\n",
    "            num_tries = idx_mod18\n",
    "            for i in range(num_tries):\n",
    "                if i == 0:\n",
    "                    if random.random() > 0.75 and len(self.words_differing_by_one_letter[gold]) > 0:\n",
    "                        tries.append(random.choice(self.words_differing_by_one_letter[gold]))\n",
    "                    else:\n",
    "                        tries.append(random.choice(self.words))\n",
    "                elif i == 1:\n",
    "                    if random.random() > 0.98 and len(self.words_differing_by_one_letter[gold]) > 0:\n",
    "                        tries.append(random.choice(self.words_differing_by_one_letter[gold]))\n",
    "                    else:\n",
    "                        tries.append(random.choice(self.words))\n",
    "                else:\n",
    "                    tries.append(random.choice(self.words))\n",
    "        elif idx_mod18 == 6:\n",
    "            tries.append(gold)\n",
    "        elif idx_mod18 < 12:\n",
    "            rare_letters = ['f', 'h', 'v', 'w', 'y', 'k', 'j', 'x', 'q', 'z']\n",
    "            rare_letters_in_gold = [char for char in rare_letters if char in gold]\n",
    "            num_tries = idx_mod18 - 6\n",
    "\n",
    "            if len(rare_letters_in_gold) == 1:\n",
    "                rare_letter = rare_letters_in_gold[0]\n",
    "                tries.append(random.choice(self.words_by_letter[rare_letter]))\n",
    "                num_tries -= 1\n",
    "            if len(rare_letters_in_gold) > 1 and num_tries > 1:\n",
    "                rare_letters = random.sample(rare_letters_in_gold, 2)\n",
    "                tries.append(random.choice(self.words_by_letter[rare_letters[0]]))\n",
    "                tries.append(random.choice(self.words_by_letter[rare_letters[1]]))\n",
    "                num_tries -= 2\n",
    "\n",
    "            for guess in range(num_tries):\n",
    "                tried_letters = []\n",
    "                for word in tries:\n",
    "                    tried_letters += list(word)\n",
    "                untried_letters = [char for char in list(string.ascii_lowercase) if char not in tried_letters]\n",
    "                new_letter = random.choice(untried_letters)\n",
    "\n",
    "                attempts = 0\n",
    "                while True:\n",
    "                    new_word = random.choice(self.words_by_letter[new_letter])\n",
    "                    num_new_letters = len([char for char in new_word if char in untried_letters])\n",
    "                    if num_new_letters > 5 - len(tries) or attempts > 9:\n",
    "                        tries.append(new_word)\n",
    "                        break\n",
    "                    attempts += 1\n",
    "\n",
    "        else:\n",
    "            num_tries = idx_mod18 - 12\n",
    "            used_letters = [char for char in gold]\n",
    "\n",
    "            for guess in range(num_tries):\n",
    "                max_used_letters = (1 + len(tries)) // 2\n",
    "\n",
    "                attempts = 0\n",
    "                while True:\n",
    "                    new_word = random.choice(self.words)\n",
    "                    new_used_letters = [char for char in new_word if char in used_letters]\n",
    "                    num_new_used_letters = len(new_used_letters)\n",
    "                    if num_new_used_letters <= max_used_letters or attempts > 9:\n",
    "                        used_letters += [char for char in new_word if char not in used_letters]\n",
    "                        tries.append(new_word)\n",
    "                        break\n",
    "                    attempts += 1\n",
    "                \n",
    "            \n",
    "        return self.tokenizer(gold, tries), gold_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c227d480-39d0-413a-a475-2c6251df06a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BucketBatchSampler(Sampler[list[int]]):\n",
    "    '''\n",
    "    Since there are only 6 possible input lengths in our dataset (corresponding to 0-5 prior guesses), training a PAD token for batching is unnecessary.\n",
    "    Rather, we use the fact that the input length of a dataset element depends only on idx % 18 to group together items we know have the same length.\n",
    "    '''\n",
    "    def __init__(self, n_samples, modulo: int, batch_size):\n",
    "        self.n_samples = n_samples\n",
    "        self.modulo = modulo #Here modulo will be 18.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        rng = random.Random()\n",
    "        buckets = [list(range(r, self.n_samples, self.modulo)) for r in range(self.modulo)]\n",
    "        for b in buckets:\n",
    "            rng.shuffle(b)\n",
    "\n",
    "        batches = []\n",
    "        for b in buckets:\n",
    "            for i in range(0, len(b), self.batch_size):\n",
    "                chunk = b[i:i + self.batch_size]\n",
    "                batches.append(chunk)\n",
    "        rng.shuffle(batches)\n",
    "\n",
    "        for batch in batches:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        total = 0\n",
    "        for r in range(self.modulo):\n",
    "            count = 0\n",
    "            if r < self.n_samples:\n",
    "                count = ((self.n_samples - 1 - r) // self.modulo) + 1\n",
    "            total += math.ceil(count / self.batch_size)\n",
    "        return total        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744f1595-3f42-476c-8f4b-c55da7f93688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    data, gold = zip(*batch)\n",
    "    return torch.stack([torch.as_tensor(t) for t in data]), torch.as_tensor(gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341c6f21-130e-49f0-b12f-2a248bc3e4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordlePlayer(nn.Module):\n",
    "    '''\n",
    "    A pretty standard transformer model. 6 layers, dimension 128, 8 attention heads. Outputs logits with length equal to self.num_words, from which the final word choice is chosen.\n",
    "    '''\n",
    "    def __init__(self, tokenizer, num_words: int):\n",
    "        super().__init__()\n",
    "        self.num_words = num_words\n",
    "        self.tokenize = tokenizer\n",
    "        \n",
    "        vocab_size = 79\n",
    "        d_model = 128\n",
    "        num_heads = 8\n",
    "        num_layers = 6\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(6, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, batch_first=True, activation=\"gelu\", norm_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.cls_head = nn.Linear(d_model, self.num_words)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.tok_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.pos_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.cls_head.weight)\n",
    "        nn.init.zeros_(self.cls_head.bias)\n",
    "\n",
    "    def next_guess(self, correct_word: str, tries_so_far: list[str], topP: float=1.0):\n",
    "        #We implement topP to avoid particularly unlikely guesses.\n",
    "        #TopP here is a better choice than topK because there's generally going to be a large number of viable early guesses, so this helps keep that variety.\n",
    "        tokenized_tries = self.tokenize(correct_word, tries_so_far)\n",
    "        x = torch.as_tensor(tokenized_tries, device=device)\n",
    "        logits = self.forward(torch.as_tensor(tokenized_tries).unsqueeze(0))\n",
    "\n",
    "        if 0.0 < topP < 1.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
    "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "            to_remove = cumulative_probs > topP\n",
    "            to_remove[..., 1:] = to_remove[..., :-1].clone()\n",
    "            to_remove[..., 0] = False\n",
    "\n",
    "            sorted_logits = sorted_logits.masked_fill(to_remove, float('-inf'))\n",
    "            filtered_logits = torch.full_like(logits, float('-inf'))\n",
    "            filtered_logits.scatter_(1, sorted_indices, sorted_logits)\n",
    "            logits = filtered_logits\n",
    "        \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        choice = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        return words[choice]\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = x.to(device)\n",
    "        _, L = x.shape\n",
    "\n",
    "        '''\n",
    "        Now here's a great little feature of this model:\n",
    "        Because the order of the guesses doesn't actually matter, all that matters per-token is the position of that letter within its guess (i.e. is it the first letter, the second, etc.)\n",
    "        So we actually only need *five* positional embeddings (plus one for the EoS token), which repeat cyclically.\n",
    "        The great thing about this is not only do we have fewer parameters to train, but also we can expect the model to run reasonably well on inputs larger than those on which it's trained.\n",
    "        The training data only features up to 5 previous guesses like real Wordle, but with these periodic embeddings, we can actually run the model on an indefinite number of prior guesses.\n",
    "        This means we can measure how long it takes to guess the correct word, even if that runs longer than the 6 guesses in our training data.\n",
    "        '''\n",
    "        pos = [0, 1, 2, 3, 4] * ((L - 1) // 5)\n",
    "        pos += [5]\n",
    "        pos = torch.as_tensor(pos).to(device)\n",
    "        h = self.tok_emb(x) + self.pos_emb(pos)\n",
    "        h = self.encoder(h)\n",
    "\n",
    "        eos_repr = h[:, -1, :]\n",
    "        logits = self.cls_head(eos_repr)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ded753c-4cff-4bbe-adfd-e4fe8bc12337",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BATCHES = 16\n",
    "\n",
    "model = WordlePlayer(tokenize, len(words)).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.0)\n",
    "warmup_steps = 100_000\n",
    "#With 16 batches, one epoch is ~15-16k steps (a bit more steps than the number of words, which here is just under 15k).\n",
    "warmup = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=lambda s: min(1.0, (s + 1) / warmup_steps))\n",
    "cosine = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=60_000, T_mult=1, eta_min=5e-5)\n",
    "#This is the scheduler that worked decently well for me.\n",
    "train_ds = WordDataset(words, tokenize)\n",
    "batch_sampler = BucketBatchSampler(n_samples=len(train_ds), modulo=18, batch_size=NUM_BATCHES)\n",
    "loader = DataLoader(train_ds, batch_sampler=batch_sampler, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e57ffa0-9f9b-4369-a2b5-318d600cc510",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1000\n",
    "#I just put a big number here and stopped it when it seemed to be plateauing. It didn't really matter with this scheduler.\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    progress = tqdm(loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    step_count = 0\n",
    "\n",
    "    for step, (tries, gold) in enumerate(progress):\n",
    "        tries = tries.to(device)\n",
    "        gold = gold.to(device)\n",
    "        \n",
    "        model_logits = model(tries)\n",
    "        loss = F.cross_entropy(model_logits, gold)\n",
    "        epoch_loss += loss.item()\n",
    "        progress.set_postfix(loss=loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        if global_step < warmup_steps:\n",
    "            warmup.step()\n",
    "        else:\n",
    "            cosine.step(global_step - warmup_steps)\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "    avg_loss = epoch_loss / len(loader)\n",
    "    print(f\"Epoch {epoch+1} | mean loss {avg_loss:.4f}\")\n",
    "\n",
    "    ckpt_path = f\"epoch_{epoch:02d}.pt\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"avg_loss\": avg_loss\n",
    "        },\n",
    "        ckpt_path\n",
    "    )\n",
    "    #The other thing with this periodic warm reset scheduler is that since the quality of the model is pretty oscillatory, it's best to just save at the end of every epoch and then check them all at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc62443-28ce-4066-8e06-5f2e7daf35de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guesses_until_correct(model, gold: str, max_guesses: int):\n",
    "    tries = []\n",
    "    num_tries = 0\n",
    "    while gold not in tries:\n",
    "        num_tries += 1\n",
    "        next_guess = model.next_guess(gold, tries, topP=0.95)\n",
    "        tries.append(next_guess)\n",
    "        if num_tries > max_guesses:\n",
    "            return tries\n",
    "    if gold in tries:\n",
    "        return tries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c784e66c-7ce2-455d-8ea6-042ca11cb785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, word_list, num_trials=1000):\n",
    "    total_guesses = 0\n",
    "    total_successes = 0\n",
    "    for j in tqdm(range(num_trials)):\n",
    "        word = random.choice(word_list)\n",
    "        num_guesses = len(guesses_until_correct(model, word, max_guesses = 100))\n",
    "        total_guesses += num_guesses\n",
    "        if num_guesses <= 6:\n",
    "            total_successes += 1\n",
    "\n",
    "    avg_guesses = total_guesses / num_trials\n",
    "    success_rate = total_successes / num_trials\n",
    "    return avg_guesses, success_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542b935e-6da9-4891-9da0-fb97d92e82c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(150): #Formatted since this way since this is how checkpoints are saved as during the training loop.\n",
    "        if i == 0:\n",
    "            ckpt_path = \"epoch_00.pt\"\n",
    "        elif i < 10:\n",
    "            ckpt_path = f\"epoch_0{i}.pt\"\n",
    "        else:\n",
    "            ckpt_path = f\"epoch_{i}.pt\"\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        model.to(device)\n",
    "        \n",
    "        avg_guesses, success_rate = evaluate_model(model, words, num_trials=1000)\n",
    "        \n",
    "        print(f\"Epoch: {i}\")\n",
    "        print(f\"Average number of guesses: {avg_guesses}\")\n",
    "        print(f\"Success rate: {success_rate}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
